{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;text-decoration: underline\">Stream Analytics with Custom Algorithms Tutorial</h1>\n",
    "<h1>Overview</h1>\n",
    "<p>Welcome to the stream analytics with custom algorithms tutorial for EpiData Lite. In this tutorial we will perform <i>custom</i> near real-time stream analytics on sample weather data acquired from a simulated wireless sensor network.</p>\n",
    "<p><b>Note:</b> This tutorial assumes the EpiData Lite platform was started with measurement-class=\"sensor_measurement\" (default) setting via conf/application.conf. If the platform was started with measurement-class=\"automated_test\" setting, please follow the tutorial in Automated Test folder.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>EpiDataLiteContext and EpiDataLiteStreamingContext</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Context and Modules Import</h3>\n",
    "<p>As a first step, We will import <i>EpiDataLiteContext</i> object <i>ec</i> and <i>EpiDataLiteStreamingContext</i> object <i>esc</i>. EpiDataLiteContext provides methods for query and offline analytics on batch data while EpiDataLiteStreamingContext provides methods for near real-time analytics on streaming data.</p> \n",
    "<p>We will also import packages and modules required for this tutorial.<p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and modules\n",
    "\n",
    "from epidata.EpiDataLiteContext import ec\n",
    "from epidata.EpiDataLiteStreamingContext import esc\n",
    "from epidata.analytics_lite import *\n",
    "\n",
    "%matplotlib inline\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math, numbers\n",
    "import json\n",
    "\n",
    "print(ec)\n",
    "print(esc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Context Initialization</h3>\n",
    "<p>Next, we initialize the EpiDataLiteContext and EpiDataLiteStreamingContext objects. This step opens the required network connections for querying, and stream processing of data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EpiDataLiteContext and EpiDataLiteStreamingContext\n",
    "\n",
    "ec.init()\n",
    "esc.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stream Analysis</h2>\n",
    "<h3>Algorithms</h3>\n",
    "<p>EpiData supports development and deployment of pre-defined as well as custom algorithms. In this tutorial, we will use custom algorithms for substituting missing (None) values and computing standard statistics.</p>\n",
    "<p>We will define custom algorithms listed below in the following code cells:\n",
    "  <ul>\n",
    "      <li><i>meas_identity()</i>: Returns the input data without any modifications or filtering</li>\n",
    "      <li><i>meas_substitute()</i>: Substitutes misssing (None) values using moving average method</li>\n",
    "      <li><i>meas_statistics()</i>: Computes standaard statistics on the measurement values, grouped by meaasurement names</li>\n",
    "  </ul>    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm for returning input data\n",
    "\n",
    "def meas_identity(df, meas_names=[], params={}):\n",
    "    \"\"\"\n",
    "    Returns input dataframe without changing it.\n",
    "    \"\"\"\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm for substituting missing measurements values with moving average values\n",
    "\n",
    "def meas_substitutes(df, meas_names, method=\"rolling\", size=3):\n",
    "    \"\"\"\n",
    "    Substitute missing measurement values within a data frame, using the specified method.\n",
    "    \"\"\"    \n",
    "    if (size % 2 == 0):\n",
    "        size = size + 1\n",
    "        \n",
    "    for meas_name in meas_names:\n",
    "        if (method == \"rolling\"): \n",
    "            if df.loc[df[\"meas_name\"]==meas_name].size > 0:\n",
    "                indices = df.loc[df[\"meas_name\"] == meas_name].index[df.loc[df[\"meas_name\"] == meas_name][\"meas_value\"].apply(\n",
    "                    lambda x: not isinstance(x, str) and (x == None or np.isnan(x)))]\n",
    "                meas_substitutes = df.loc[df[\"meas_name\"]==meas_name][\"meas_value\"].rolling( window=size, min_periods=1, center=True).mean()\n",
    "            \n",
    "                df[\"meas_value\"].fillna(meas_substitutes, inplace=True)\n",
    "                df.loc[indices, \"meas_flag\"] = \"substituted\"\n",
    "                df.loc[indices, \"meas_method\"] = \"rolling average\"\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported substitution method: \", repr(method))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms for computing statistics on measurements \n",
    "\n",
    "def subgroup_statistics(row):\n",
    "    \"\"\"\n",
    "    Compute standard statistics on a sub-group of measurements.\n",
    "    \"\"\"\n",
    "    row['start_time'] = np.min(row[\"ts\"])\n",
    "    row[\"stop_time\"] = np.max(row[\"ts\"])\n",
    "    row[\"meas_summary_name\"] = \"statistics\"\n",
    "    row[\"meas_summary_value\"] = json.dumps({'count': int(row[\"meas_value\"].count()), 'mean': row[\"meas_value\"].mean(),\n",
    "                                            'std': row[\"meas_value\"].std(), 'min': row[\"meas_value\"].min(), \n",
    "                                            'max': row[\"meas_value\"].max()})\n",
    "    row[\"meas_summary_description\"] = \"descriptive statistics\"\n",
    "    return row\n",
    "\n",
    "def meas_statistics(df, meas_names, method=\"standard\"):\n",
    "    \"\"\"\n",
    "    Compute statistics on measurement values using the specified method. The measurements are grouped by measurement name.\n",
    "    \"\"\"\n",
    "    if (method == \"standard\"):\n",
    "        df_grouped = df.loc[df[\"meas_name\"].isin(meas_names)].groupby([\"company\", \"site\", \"station\", \"sensor\", \"event\", \"meas_name\"], \n",
    "                            as_index=False)\n",
    "        df_summary = df_grouped.apply(subgroup_statistics).loc[:, [\"company\", \"site\", \"station\", \"sensor\",\n",
    "            \"start_time\", \"stop_time\", \"event\", \"meas_name\", \"meas_summary_name\", \"meas_summary_value\", \n",
    "            \"meas_summary_description\"]].drop_duplicates()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported summary method: \", repr(method))\n",
    "                \n",
    "    return df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transformations</h3>\n",
    "<p>Next, we will define transformations using custom algorithms defined above. EpiData transformations are created using method <i>create_transformation()</i>, which takes the following inputs:\n",
    "    <ul>\n",
    "    <li>Pre-defined or custom algorithm</li>\n",
    "    <li>List of measurements to apply the algorithm to</li>\n",
    "    <li>Arguments for the algorithm</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tranformations using custom algorithms\n",
    "\n",
    "op1 = esc.create_transformation(meas_substitutes, [\"Temperature\", \"Wind_Speed\", \"Relative_Humidity\"], {\"method\":\"rolling\", \"size\":3})\n",
    "op2 = esc.create_transformation(meas_identity, [], {})\n",
    "op3 = esc.create_transformation(meas_statistics, [\"Temperature\", \"Wind_Speed\", \"Relative_Humidity\"], {\"method\": \"standard\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Streams</h3>\n",
    "<p>Once transformations have been created, we define streams using EpiDataLiteStreamingContext's <i>create_stream()</i> method. The method takes source topic, destination topic, and transformation object as its inputs. The source topic can be one of the pre-defined topics, namely <i>measurements_original</i>, or a custom topic defined by you, for example <i>'measurements_substituted'</i>. The destination topic can be one of the pre-defined topics, namely <i>'measurements_cleansed'</i> and <i>'measurements_summary'</i>, or a custom topic defined by you, for example, <i>'measurements_substituted'</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stream processing\n",
    "\n",
    "esc.create_stream(\"measurements_original\", \"measurements_substituted\", op1)\n",
    "esc.create_stream(\"measurements_substituted\", \"measurements_cleansed\", op2)\n",
    "esc.create_stream(\"measurements_substituted\", \"measurements_summary\", op3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next, we start the streams using EpiDataLiteStreamingContext's <i>start_streaming()</i> method. This starts the transformation operations on near real-time data and sends the results to the specified destinations</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start near real-time processing\n",
    "\n",
    "esc.start_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Ingestion</h2>\n",
    "\n",
    "<h3>1. Download Python Script</h3>\n",
    "\n",
    "<p>We will use the provided Python script <i>sensor_data_ingest_with_outliers.py</i> to simulate weather data and push it to the EpiData Lite platform. Download the example <i>sensor_data_ingest_with_outliers.py</i> from Jupyter Notebook's tree view as show below.</p>\n",
    "<img src=\"./static/jupyter_tree_view.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Ingest Data using Terminal / Command Prompt</h3>\n",
    "<p>The next step is to run the Python script <i>'sensor_data_ingest_with_outliers.py'</i> using a Python 3 interpreter. This script sends simulated weather data to EpiData Lite platform using REST interface. You should see status of each ingestion steps (iterations) in your standard output.</p>\n",
    "<p>For brewity, we have only included the output of first iteration in the image below:</p>\n",
    "<img src=\"./static/terminal_view_with_output.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Query, Retrieve and Visualization</h2>\n",
    "\n",
    "<h3>1. Query - Keys</h3>\n",
    "<p>Data stored in the EpiData platform can be queried by specifying the primary data attributes, start time and stop time. Below are the primary data attributes for the current dataset:\n",
    "<ul>\n",
    "<li><i>company, site, station, sensor</i></li>\n",
    "</ul>\n",
    "</p>\n",
    "<p>\n",
    "We can use EpiDataLiteContext's <i>list_keys()</i> method to obtain the values of the primary data attributes for our simulated weather dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the primary data attributes\n",
    "\n",
    "keys = ec.list_keys()\n",
    "\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Query - Original, Cleansed and Summary Data</h3>\n",
    "<p>We will query original and processed data using EpiDataLiteContext's <i>query_measurements_original()</i>, <i>query_measurements_cleansed()</i> and <i>query_measurements_summary()</i> methods.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query original measurements\n",
    "\n",
    "primary_key={\"company\": \"EpiData\", \"site\": \"San_Francisco\", \"station\":\"WSN-1\", \"sensor\": [\"Temperature_Probe\",\"Anemometer\",\"RH_Probe\"]}\n",
    "start_time = datetime.strptime('01/01/2023 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "stop_time = datetime.strptime('01/01/2024 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "df_original = ec.query_measurements_original(primary_key, start_time, stop_time)\n",
    "\n",
    "print(df_original.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query cleansed measurements\n",
    "\n",
    "primary_key={\"company\": \"EpiData\", \"site\": \"San_Francisco\", \"station\":\"WSN-1\", \"sensor\": [\"Temperature_Probe\",\"Anemometer\",\"RH_Probe\"]}\n",
    "start_time = datetime.strptime('1/1/2023 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "stop_time = datetime.strptime('1/1/2024 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "df_cleansed = ec.query_measurements_cleansed(primary_key, start_time, stop_time)\n",
    "\n",
    "print(df_cleansed.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query measurements summary\n",
    "\n",
    "primary_key={\"company\": \"EpiData\", \"site\": \"San_Francisco\", \"station\":\"WSN-1\", \"sensor\": [\"Temperature_Probe\",\"Anemometer\",\"RH_Probe\"]}\n",
    "start_time = datetime.strptime('1/1/2023 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "stop_time = datetime.strptime('1/1/2024 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "df_summary = ec.query_measurements_summary(primary_key, start_time, stop_time)\n",
    "print(df_summary.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Visualize - Original and Cleansed Data</h3>\n",
    "\n",
    "<p>Next, we will visualize the original and cleansed data using Python's Bokeh package. In the resulting visualization, we can see the result of <i>'identity'</i> and <i>'meas_substitutes'</i> algorithms applied to the original data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Stream Analytics Results\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bokeh.layouts import column\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "df_original_temperatures = df_original.loc[df_original[\"meas_name\"] == \"Temperature\"]\n",
    "df_cleansed_temperatures = df_cleansed.loc[df_cleansed[\"meas_name\"] == \"Temperature\"]\n",
    "\n",
    "plot_original = figure(min_width=800, height=200, x_axis_label=\"Timestamp\", x_axis_type=\"datetime\", y_axis_label=\"Temperature\")\n",
    "plot_original.background_fill_color = \"#fafafa\"\n",
    "plot_original.line(df_original_temperatures[\"ts\"], df_original_temperatures[\"meas_value\"], color='navy', alpha=0.75)\n",
    "plot_original.title = \"Original Measurements\"\n",
    "\n",
    "plot_cleansed = figure(min_width=800, height=200, x_axis_label=\"Timestamp\", x_axis_type=\"datetime\", y_axis_label=\"Temperature\")\n",
    "plot_cleansed.background_fill_color = \"#fafafa\"\n",
    "plot_cleansed.line(df_cleansed_temperatures[\"ts\"], df_cleansed_temperatures[\"meas_value\"], color='red', alpha=0.75)\n",
    "plot_cleansed.title = \"Cleansed Measurements\"\n",
    "\n",
    "show(column(plot_original, plot_cleansed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stop Stream Analytics</h2>\n",
    "<p>We can now stop the stream processig using EpiDataLiteStreamingContext's <i>stop_streaming()</i> method.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop current near real-time processing\n",
    "\n",
    "esc.stop_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Context Closing</h2>\n",
    "<p>Now, we can clear (reset) the EpiDataLiteContext and EpiDataLiteStreamingContext using their respective <i>clear</i> methods.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear EpiDataLiteContext and EpiDataLiteStreamingContext\n",
    "\n",
    "ec.clear()\n",
    "esc.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Next Steps</h2>\n",
    "<p>Congratulations, you have successfully perfomed near real-time analytics using custom algorithms on simulated weather data. The next step is to explore various capabilities of EpiData by creating your own near real-time analytics application!</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "read_only": false
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
