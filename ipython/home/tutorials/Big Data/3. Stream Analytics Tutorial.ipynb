{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;text-decoration: underline\">Stream Analytics Tutorial</h1>\n",
    "<h1>Overview</h1>\n",
    "<p>Welcome to the stream analytics tutorial for EpiData. In this tutorial we will perform near real-time stream analytics on sample weather data acquired from a simulated wireless sensor network.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Package and Module Imports</h2>\n",
    "<p>As a first step, we will import packages and modules required for this tutorial. Since <i>EpiData Context (ec)</i> is required to use the application, it is implicitly imported. Sample functions for near real-time analytics are avaialable in <i>EpiData Analytics</i> package. Other packages and modules, such as <i>datetime</i>, <i>pandas</i> and <i>matplotlib</i>, can also be imported at this time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from epidata.context import ec\n",
    "from epidata.analytics import *\n",
    "\n",
    "%matplotlib inline\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stream Analysis</h2>\n",
    "<h3>Function Definition</h3>\n",
    "<p>EpiData supports development and deployment of custom algorithms via Jupyter Notebook. Below, we define python functions for substituting extreme outliers and aggregating temperature measurements. These functions can be operated on near real-time and historic data. In this tutorial, we will apply the functions on near real-time data available from Kafka 'measurements' and 'measurements_cleansed' topics</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math, numbers\n",
    "\n",
    "def substitute_demo(df, meas_names, method=\"rolling\", size=3):\n",
    "    \"\"\"\n",
    "    Substitute missing measurement values within a data frame, using the specified method.\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"meas_value\"].replace(250, np.nan, inplace=True)\n",
    "    \n",
    "    for meas_name in meas_names:\n",
    "\n",
    "        if (method == \"rolling\"):\n",
    "            if ((size % 2 == 0) and (size != 0)): size += 1   \n",
    "            if df.loc[df[\"meas_name\"]==meas_name].size > 0:\n",
    "                indices = df.loc[df[\"meas_name\"] == meas_name].index[df.loc[df[\"meas_name\"] == meas_name][\"meas_value\"].apply(\n",
    "                    lambda x: not isinstance(x, basestring) and (x == None or np.isnan(x)))]\n",
    "                substitutes = df.loc[df[\"meas_name\"]==meas_name][\"meas_value\"].rolling( window=size, min_periods=1, center=True).mean()\n",
    "            \n",
    "                df[\"meas_value\"].fillna(substitutes, inplace=True)\n",
    "                df.loc[indices, \"meas_flag\"] = \"substituted\"\n",
    "                df.loc[indices, \"meas_method\"] = \"rolling average\"\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported substitution method: \", repr(method))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def subgroup_statistics(row):\n",
    "    row['start_time'] = np.min(row[\"ts\"])\n",
    "    row[\"stop_time\"] = np.max(row[\"ts\"])\n",
    "    row[\"meas_summary_name\"] = \"statistics\"\n",
    "    row[\"meas_summary_value\"] = json.dumps({'count': row[\"meas_value\"].count(), 'mean': row[\"meas_value\"].mean(),\n",
    "                                            'std': row[\"meas_value\"].std(), 'min': row[\"meas_value\"].min(), \n",
    "                                            'max': row[\"meas_value\"].max()})\n",
    "    row[\"meas_summary_description\"] = \"descriptive statistics\"\n",
    "    return row\n",
    "\n",
    "def meas_statistics_demo(df, meas_names, method=\"standard\"):\n",
    "    \"\"\"\n",
    "    Compute statistics on measurement values within a data frame, using the specified method.\n",
    "    \"\"\"\n",
    "    \n",
    "    if (method == \"standard\"):\n",
    "        df_grouped = df.loc[df[\"meas_name\"].isin(meas_names)].groupby([\"company\", \"site\", \"station\", \"sensor\"], \n",
    "                            as_index=False)\n",
    "        df_summary = df_grouped.apply(subgroup_statistics).loc[:, [\"company\", \"site\", \"station\", \"sensor\",\n",
    "            \"start_time\", \"stop_time\", \"event\", \"meas_name\", \"meas_summary_name\", \"meas_summary_value\", \n",
    "            \"meas_summary_description\"]].drop_duplicates()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported summary method: \", repr(method))\n",
    "                \n",
    "    return df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transformations and Streams</h3>\n",
    "<p>The analytics algorithms are executed on near real-time data through transformations. A transformation specifies the function, its parameters and destination. The destination can be one of the database tables, namely <i>'measurements_cleansed'</i> or <i>'measurements_summary'</i>, or another Kafka topic.</p>\n",
    "<p>Once the transformations are defined, they are initiated via <i>ec.create_stream(transformations, data_source, batch_duration)</i> function call.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stop current near real-time processing\n",
    "ec.stop_streaming()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define tranformations and steam operations\n",
    "op1 = ec.create_transformation(substitute_demo, [[\"Temperature\", \"Wind_Speed\"], \"rolling\", 3], \"measurements_substituted\")\n",
    "ec.create_stream([op1], \"measurements\")\n",
    "\n",
    "op2 = ec.create_transformation(identity, [], \"measurements_cleansed\")\n",
    "op3 = ec.create_transformation(meas_statistics, [[\"Temperature\", \"Wind_Speed\"], \"standard\"], \"measurements_summary\")\n",
    "ec.create_stream([op2, op3],\"measurements_substituted\")\n",
    "\n",
    "# Start near real-time processing\n",
    "ec.start_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Ingestion</h3>\n",
    "<p>We can now start data ingestion from simulated wireless sensor network. To do so, you can download and run the <i>sensor_data_with_outliers.py</i> example shown in the image below.</p>\n",
    "<img src=\"./static/jupyter_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Query and Visualization</h3>\n",
    "<p>We query the original and processed data from Kafka queue using Kafka Consumer. The data obtained from the quey is visualized using Bokeh charts.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "from bokeh.layouts import row, column\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot1 = figure(plot_width=750, plot_height=200, x_axis_type='datetime', y_range=(30, 300))\n",
    "plot2 = figure(plot_width=750, plot_height=200, x_axis_type='datetime', y_range=(30, 300))\n",
    "df_kafka_init = pd.DataFrame(columns = [\"ts\", \"meas_value\"])\n",
    "test_data_1 = ColumnDataSource(data=df_kafka_init.to_dict(orient='list'))\n",
    "test_data_2 = ColumnDataSource(data=df_kafka_init.to_dict(orient='list'))\n",
    "meas_name = \"Temperature\"\n",
    "\n",
    "plot1.circle(\"ts\", \"meas_value\", source=test_data_1, legend=meas_name, line_color='orangered', line_width=1.5)\n",
    "line1 = plot1.line(\"ts\", \"meas_value\", source=test_data_1, legend=meas_name, line_color='orangered', line_width=1.5)\n",
    "plot1.legend.location = \"top_right\"\n",
    "plot2.circle(\"ts\", \"meas_value\", source=test_data_2, legend=meas_name, line_color='blue', line_width=1.5)\n",
    "line2 = plot2.line(\"ts\", \"meas_value\", source=test_data_2, legend=meas_name, line_color='blue', line_width=1.5)\n",
    "plot2.legend.location = \"top_right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer()\n",
    "consumer.subscribe(['measurements', 'measurements_substituted'])\n",
    "delay = .1\n",
    "\n",
    "handle = show(column(plot1, plot2), notebook_handle=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in consumer:\n",
    "    topic = message.topic\n",
    "    measurements = json.loads(message.value)\n",
    "    df_kafka = json_normalize(measurements)\n",
    "    df_kafka[\"meas_value\"] = np.nan if \"meas_value\" not in measurements else measurements[\"meas_value\"]\n",
    "    df_kafka = df_kafka.loc[df_kafka[\"meas_name\"]==meas_name]    \n",
    "    df_kafka = df_kafka[[\"ts\", \"meas_value\"]]\n",
    "    df_kafka[\"ts\"] = df_kafka[\"ts\"].apply(lambda x: pd.to_datetime(x, unit='ms').tz_localize('UTC').tz_convert('US/Pacific')) \n",
    "    \n",
    "    if (not df_kafka.empty):\n",
    "        if (topic == 'measurements'):\n",
    "            test_data_1.stream(df_kafka.to_dict(orient='list'))\n",
    "        if (topic == 'measurements_substituted'):\n",
    "            test_data_2.stream(df_kafka.to_dict(orient='list'))\n",
    "        push_notebook(handle=handle)\n",
    "        \n",
    "    time.sleep(delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Another way to query and visualize processed data is using <i>ec.query_measurements_cleansed(..) and ec.query_measurements_summary(..)</i> functions. For our example, we specify paramaters that match sample data set, and query the aggregated values using <i>ec.query_measurements_summary(..)</i> function call.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY MEASUREMENTS_CLEANSED TABLE\n",
    "\n",
    "primary_key={\"company\": \"EpiData\", \"site\": \"San_Jose\", \"station\":\"WSN-1\", \n",
    "             \"sensor\": [\"Temperature_Probe\", \"RH_Probe\", \"Anemometer\"]}\n",
    "start_time = datetime.strptime('8/19/2017 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "stop_time = datetime.strptime('8/20/2017 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "df_cleansed = ec.query_measurements_cleansed(primary_key, start_time, stop_time)\n",
    "print \"Number of records:\", df_cleansed.count()\n",
    "\n",
    "df_cleansed_local = df_cleansed.toPandas()\n",
    "df_cleansed_local[df_cleansed_local[\"meas_name\"]==\"Temperature\"].tail(10).sort_values(by=\"ts\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY MEASUREMNTS_SUMMARY TABLE\n",
    "\n",
    "primary_key={\"company\": \"EpiData\", \"site\": \"San_Jose\", \"station\":\"WSN-1\", \"sensor\": [\"Temperature_Probe\"]}\n",
    "start_time = datetime.strptime('8/19/2017 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "stop_time = datetime.strptime('8/20/2017 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "last_index = -1\n",
    "summary_result = pd.DataFrame()\n",
    "\n",
    "df_summary = ec.query_measurements_summary(primary_key, start_time, stop_time)\n",
    "df_summary_local = df_summary.toPandas()\n",
    "summary_keys = df_summary_local[[\"company\", \"site\", \"station\", \"sensor\", \"start_time\", \"stop_time\", \"meas_name\", \"meas_summary_name\"]]\n",
    "summary_result = df_summary_local[\"meas_summary_value\"].apply(json.loads).apply(pd.Series)\n",
    "summary_combined = pd.concat([summary_keys, summary_result], axis=1)\n",
    "\n",
    "summary_combined.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stop Stream Analytics</h3>\n",
    "<p>The transformations can be stopped at any time via <i>ec.stop_streaming()</i> function call<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stop current near real-time processing\n",
    "ec.stop_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Next Steps</h2>\n",
    "<p>Congratulations, you have successfully perfomed near real-time analytics on sample data aquired by a simulated wireless sensor network. The next step is to explore various capabilities of EpiData by creating your own custom analytics application!</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EpiData PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "read_only": false
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
